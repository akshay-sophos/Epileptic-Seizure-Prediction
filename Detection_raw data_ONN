
import numpy as np
import time
import pandas as pd
from keras.utils import to_categorical
#from keras.metrics import confusion_matrix
from keras import regularizers,optimizers
from keras.models import Sequential
from keras.layers import Dense,Dropout
from keras.layers.normalization import BatchNormalization
from keras.utils.vis_utils import plot_model
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
import glob
#Variable Declaration
THRESHOLD = 0.5     # Threshold for predicting seizure
lambda_reg = 0.001  # Lambda for regression
l_rate = 0.01       # Learning rate for gradient descent
l_rate_decay = 0.1  # Decay of learning rate over epoch
p_dropout = 0.4     # Probability of dropout
batch_size= 100000  # Size of each batch
valid_split = 0     # Size of validation
epoch     = 9       # Epochs(meaning no of times we train the same data)
hip       = 23      # Input of the neural network
h1        = 30
h2        = 30
h3        = 20
h4        = 30
hop       = 3
#%%
FAILURE WITH F1 SCORE 20% ONLY
#def f1score(y_true, y_pred):
#    return keras.backend.(y_pred) 
#%%
#Should we use BatchNormalization before or after activation function??????
classifier = Sequential()
#Adding input layer and first hidden layer
classifier.add(Dense(h1, activation = 'relu',use_bias=True,kernel_regularizer=regularizers.l2(lambda_reg), input_shape = (None,hip)))
classifier.add(BatchNormalization())
classifier.add(Dropout(p_dropout))
#Adding second hidden layer
classifier.add(Dense(h2, use_bias=True,kernel_regularizer=regularizers.l2(lambda_reg),activation = 'relu'))
classifier.add(BatchNormalization())
classifier.add(Dropout(p_dropout))
#Adding third hidden layer
classifier.add(Dense(h3, use_bias=True,kernel_regularizer=regularizers.l2(lambda_reg),activation = 'relu'))
classifier.add(BatchNormalization())
classifier.add(Dropout(p_dropout))
#4th layer
classifier.add(Dense(h4, use_bias=True,activation = 'relu'))
classifier.add(BatchNormalization())
classifier.add(Dropout(p_dropout))
#Adding the output layer
classifier.add(Dense(units =hop, use_bias=True, activation = 'softmax'))
adam = optimizers.Adam(lr=l_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=l_rate_decay, amsgrad=False)
#Compiling the ANN
#%%
path = "D:\seizure data\seizureddata(0,1,2)"
file = glob.glob(path+'\chb_'+'[0-9][0-9]'+'_'+'[0-9][0-9]'+'.csv')
#%%
#file = [f for f in os.listdir(path) if re.match(r'chb_01_[0-9]*[0-9].csv', f)]
dat = np.empty([0,25])
for i in sorted(file):
    t1 = time.time();
    n = pd.read_csv(i,header=None)# We can definitely remove the variable by replacing n in the next line
    print(n.shape,dat.shape)
    dat = np.vstack((dat,n))
    t2 = time.time()
    print(dat.shape)
    print("Time for "+str(i[-6:-4])+" is "+str(t2-t1)+"sec")
print('............Data Loaded.................')
# %%
df_y = to_categorical( dat[:,-1],num_classes=3)
X_train, X_test, y_train, y_test = train_test_split(dat[:,:-2],df_y, test_size = 0.3, random_state = 0)
print('............Data Split..................')
#%%
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
X_train = np.expand_dims(X_train, axis=1) 
X_test = np.expand_dims(X_test, axis=1) 
y_train = np.expand_dims(y_train, axis=1) 
#y_train = np.expand_dims(y_train, axis=1) 
y_test = np.expand_dims(y_test, axis=1) 
#y_test = np.expand_dims(y_test, axis=1) 
print('............Data Normalized.............')
print(X_train.shape)
print(y_train.shape)
# %%
classifier.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])#, f1_score])#, 'precision', 'recall'])
#classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
#Fitting the ANN to the training set
history = classifier.fit(X_train, y_train, validation_split=valid_split, batch_size = batch_size, epochs = epoch, verbose=1)
print('............Data Trained................')
# %%Predicting the Train set results
score = classifier.evaluate(X_train, y_train, verbose=1)
print('Test score:', score[0])
print('Test accuracy:', score[1])
y_pred = classifier.predict(X_train,verbose=1)
#y_pred = (y_pred > THRESHOLD).astype(float)
#y_pred = np.squeeze(y_pred)
#y_train = np.squeeze(y_train)
#y_pred = y_pred.reshape(np.size(y_pred))
#y_train = y_train.reshape(np.size(y_train))
# Making the Confusion Matrix
print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1),labels=[0,1,2]))
#print("Precision",precision_score(y_train, y_pred,labels=[0,1,2]))
#print("Recall",recall_score(y_train, y_pred,labels=[0,1,2]))
#print("F1",f1_score(y_train, y_pred,labels=[0,1,2]))
print('............Test Over...................')


#%%Predicting the Test set results
y_pred = classifier.predict(X_test,verbose=1)
y_pred = (y_pred > THRESHOLD).astype(float)
y_pred = np.squeeze(y_pred)
y_test = np.squeeze(y_test)
# Making the Confusion Matrix
print(confusion_matrix(y_test,y_pred,labels=[0,1]))
print("Precision",precision_score(y_test, y_pred,labels=[0,1]))
print("Recall",recall_score(y_test, y_pred,labels=[0,1]))
print("F1",f1_score(y_test, y_pred,labels=[0,1]))
print('............Test Over...................')

#%%
plot_model(classifier, to_file='D:\Major\model_plot.png', show_shapes=True, show_layer_names=True)
# Plot training & validation accuracy values
plt.plot(history.history['acc'])
#plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
#plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

#ann_viz(classifier, view=True, filename='net.gv', title='Neural Network')
