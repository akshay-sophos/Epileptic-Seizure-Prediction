import numpy as np
import time
import pandas as pd
from keras import regularizers,optimizers
from keras.models import Sequential,model_from_json
from keras.layers import Dense,Dropout,CuDNNLSTM,LSTM,TimeDistributed,GlobalAveragePooling1D
from keras.layers.normalization import BatchNormalization
from keras.utils.vis_utils import plot_model
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
import glob
from ann_visualizer.visualize import ann_viz
np.random.seed(9)
#Variable Declaration
THRESHOLD = 0.5   # Threshold for predicting seizure
lambda_reg = 1     # Lambda for regression
l_rate = 0.001      # Learning rate for gradient descent
l_rate_decay = 1   # Decay of learning rate over epoch
p_dropout = 0.3    # Probability of dropout
batch_size= 200   # Size of each batch
valid_split = 0.2  # Size of validation
epoch     = 200   # Epochs(meaning no of times we train the same data)
hip       = 67     # Input of the neural network
h1        = 20
h2        = 20
h3        = 20
h4        = 20
hop       = 1
t = 23

#Try changing threshold to 0.2 and cost from 1000 to 10K 
#SGD
#Whats learning rate_decay
#Do with only waVELET FEATURES
#Do with raw data

adam = optimizers.Adam(lr=l_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=l_rate_decay, amsgrad=False)
from keras.optimizers import SGD
opt = SGD(lr=l_rate,decay=l_rate_decay,momentum = 0.8)
#%%
classifier = Sequential()
#Adding input layer and first hidden layer
classifier.add(CuDNNLSTM(h1,return_sequences=True,kernel_regularizer=regularizers.l2(lambda_reg), batch_input_shape = (None,t,hip)))
classifier.add(BatchNormalization())
classifier.add(Dropout(p_dropout))
#Adding second hidden layer
classifier.add(CuDNNLSTM(h2, return_sequences=True,kernel_regularizer=regularizers.l2(lambda_reg),))
classifier.add(BatchNormalization())
classifier.add(Dropout(p_dropout))
classifier.add(CuDNNLSTM(h3,return_sequences=True,kernel_regularizer=regularizers.l2(lambda_reg), batch_input_shape = (None,t,hip)))
classifier.add(BatchNormalization())
classifier.add(Dropout(p_dropout))
#Adding second hidden layer
classifier.add(CuDNNLSTM(h4, return_sequences=True,kernel_regularizer=regularizers.l2(lambda_reg),))
classifier.add(BatchNormalization())
classifier.add(Dropout(p_dropout))
###Adding third hidden layer
#classifier.add(CuDNNLSTM(h3, return_sequences=True,kernel_regularizer=regularizers.l2(lambda_reg),))
#classifier.add(BatchNormalization())
#classifier.add(Dropout(p_dropout))
#4th layer
#classifier.add(TimeDistributed(Dense(h4)))#, return_sequences=True,))
#classifier.add(BatchNormalization())
#classifier.add(Dropout(p_dropout))
#Adding the output layer
classifier.add(GlobalAveragePooling1D())

classifier.add(Dense(units =hop,  activation = 'sigmoid'))
#,clipvalue=0.5,clipnorm=1)
#plot_model(classifier, to_file='model_plot.png', show_shapes=True, show_layer_names=True)
#ann_viz(classifier, view=True, filename='net.gv', title='Neural Network')
classifier.summary()
#Compiling the ANN

import keras.backend as K

def wloss(y_true, y_pred):
    if(y_true==1 and y_pred==0):
        return K.binary_crossentropy(y_true,y_pred)*3000
    else:
        return K.binary_crossentropy(y_true,y_pred)*1

#%%
path = "D:\seizure data\dan"
# file = glob.glob(path+'\chb_'+'[0-9][5]'+'_'+'[0-9][0-9]'+'_train'+'.csv')
file = glob.glob(path+'\chb_05_17_train.csv')
# NOT NEEDED AS we arent using one hot df_y = to_categorical( dat[:,-2],num_classes=3)
dat = np.empty([0,hip+2])
for i in sorted(file):
    t1 = time.time();
    n = pd.read_csv(i,header=None)# We can definitely remove the variable by replacing n in the next line
    print(n.shape,dat.shape)
    dat = np.vstack((dat,n))
    t2 = time.time()
    print(dat.shape)

#file = glob.glob(path+'\chb_'+'[0-9][5]'+'_'+'[0-9][0-9]'+'_test'+'.csv')
file = glob.glob(path+'\chb_05_17_test.csv')
# NOT NEEDED AS we arent using one hot df_y = to_categorical( dat[:,-2],num_classes=3)
dat1 = np.empty([0,hip+2])
for i in sorted(file):
    t1 = time.time();
    n = pd.read_csv(i,header=None)# We can definitely remove the variable by replacing n in the next line
    print(n.shape,dat.shape)
    dat1 = np.vstack((dat1,n))
    t2 = time.time()
    print(dat1.shape)
    print("Time for "+str(i[-13:-4])+" is "+str(t2-t1)+"sec")
print('............Data Loaded.................')

X_train  = dat[:,:-2]
X_test =  dat1[:,:-2]
y_train = dat[:,-2]
y_test = dat1[:,-2]
print('............Data Split..................')

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
a=np.shape(X_train)[0]/(t)
b=np.shape(X_test)[0]/t
X_train = X_train[0:int(a)*23]
X_test = X_test[0:int(b)*23]
y_train = y_train[0:int(a)*23]
y_test = y_test[0:int(b)*23]
X_train = X_train.reshape(int(a),23,hip).astype('float32')
X_test = X_test.reshape(int(b),23,hip).astype('float32')
#X_train = np.expand_dims(X_train, axis=1) 
#X_test = np.expand_dims(X_test, axis=1) 
y_train = np.expand_dims(y_train, axis=1)[0::23] 
#y_train = np.expand_dims(y_train, axis=1) 
#y_test = np.expand_dims(y_test, axis=1) 
y_test = np.expand_dims(y_test, axis=1)[0::23] 
print('............Data Normalized.............')
print(X_train.shape)
print(y_train.shape)

# %%
classifier.compile(optimizer = "adam", loss = wloss, metrics = ['accuracy'])
#Fitting the ANN to the training set
history = classifier.fit(X_train, y_train, validation_split=valid_split, batch_size = batch_size, epochs = epoch, verbose=1)
print('............Data Trained................')


# %%Predicting the Train set results
y_pred = classifier.predict(X_train,verbose=1)
y_pred = (y_pred > THRESHOLD).astype(float)
y_pred = np.squeeze(y_pred)
y_train = np.squeeze(y_train)
#
#y_pred = y_pred.reshape(np.size(y_pred))
#y_train = y_train.reshape(np.size(y_train))
# Making the Confusion Matrix
print(confusion_matrix(y_train,y_pred,labels=[0,1])) 
print("Precision",precision_score(y_train, y_pred,labels=[0,1]))
print("Recall",recall_score(y_train, y_pred,labels=[0,1]))
print("F1",f1_score(y_train, y_pred,labels=[0,1]))
print('............Test Over...................')

y_pred = classifier.predict(X_test,verbose=1)
y_pred = (y_pred > THRESHOLD).astype(float)
y_pred = np.squeeze(y_pred)
y_test = np.squeeze(y_test)
# Making the Confusion Matrix
print(confusion_matrix(y_test,y_pred,labels=[0,1]))
print("Precision",precision_score(y_test, y_pred,labels=[0,1]))
print("Recall",recall_score(y_test, y_pred,labels=[0,1]))
print("F1",f1_score(y_test, y_pred,labels=[0,1]))
print('............Test Over...................')

#%%
plot_model(classifier, to_file='D:\Major\model_plot.png', show_shapes=True, show_layer_names=True)
# Plot training & validation accuracy values
plt.plot(history.history['acc'])
#plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
#plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

#ann_viz(classifier, view=True, filename='net.gv', title='Neural Network')
#%%
# serialize model to JSON
classifier_json = classifier.to_json()
with open("classifier3.json", "w") as json_file:
    json_file.write(classifier_json)
# serialize weights to HDF5
classifier.save_weights("classifierask_with_chb01.h5")
print("Saved model to disk")
#%%
# load json and create model
json_file = open('classifier3.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
classifier = model_from_json(loaded_model_json)
# load weights into new model
classifier.load_weights("classifierask.h5")
print("Loaded model from disk")
#%%
# load json and create model
json_file = open('classifierl9.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
classifier = model_from_json(loaded_model_json)
# load weights into new model
classifier.load_weights("classifierask.h5")
print("Loaded model from disk")
#%%
def reset_weights(classifier):
    session = K.get_session()
    for layer in classifier.layers: 
        if hasattr(layer, 'kernel_initializer'):
            layer.kernel.initializer.run(session=session)
reset_weights(classifier)
